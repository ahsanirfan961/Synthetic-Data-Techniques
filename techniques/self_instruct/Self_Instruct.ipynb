{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htonDIVhCO5y",
        "outputId": "08fc6e25-be8d-459c-ecac-721c3d8f12a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: distilabel>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distilabel[hf-transformers,openai]>=1.0.0) (1.3.2)\n",
            "Requirement already satisfied: datasets>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.21.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.1.4)\n",
            "Requirement already satisfied: multiprocess>=0.70 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.70.16)\n",
            "Requirement already satisfied: nest-asyncio>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.3)\n",
            "Requirement already satisfied: orjson>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.10.7)\n",
            "Requirement already satisfied: portalocker>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.10.1)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.8.2)\n",
            "Requirement already satisfied: rich>=13.5.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (13.7.1)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.13.1)\n",
            "Requirement already satisfied: tblib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.12.4)\n",
            "Requirement already satisfied: universal-pathlib>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.2.3)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from distilabel[hf-transformers,openai]>=1.0.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.34.1 in /usr/local/lib/python3.10/dist-packages (from distilabel[hf-transformers,openai]>=1.0.0) (4.42.4)\n",
            "Collecting openai>=1.0.0 (from distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Downloading openai-1.42.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.1.4)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.5.0)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (6.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.14.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=3.1.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.1.5)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.20.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.5.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.13.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.1->distilabel[hf-transformers,openai]>=1.0.0) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.1->distilabel[hf-transformers,openai]>=1.0.0) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.34.1->distilabel[hf-transformers,openai]>=1.0.0) (0.19.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.5.4)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.25.2->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.2.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.16.0->distilabel>=1.0.0->distilabel[hf-transformers,openai]>=1.0.0) (1.16.0)\n",
            "Downloading openai-1.42.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.9/362.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m740.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jiter, nvidia-cusparse-cu12, nvidia-cudnn-cu12, openai, nvidia-cusolver-cu12\n",
            "Successfully installed jiter-0.5.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 openai-1.42.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"distilabel[hf-transformers, openai]>=1.0.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6neAPmBvCN0M"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict, Dataset\n",
        "import pandas as pd\n",
        "from distilabel.llms import TransformersLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub, KeepColumns, LoadDataFromDicts\n",
        "from distilabel.steps import Step, StepInput\n",
        "from distilabel.steps.typing import StepOutput\n",
        "from distilabel.steps.tasks import TextGeneration, SelfInstruct\n",
        "from typing import List\n",
        "from pydantic import Field"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yE1b6u4DDRa",
        "outputId": "68ac925b-8013-4fa0-ccc5-cc572bb257cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "HF_AUTH_TOKEN='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'\n",
        "from huggingface_hub import login\n",
        "login(token=HF_AUTH_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K2WiKo3_Lv3L"
      },
      "outputs": [],
      "source": [
        "criteria_for_query_generation = (\n",
        "    \"1. Relevance: Ensure the questions are directly related to the content and context of the input paragraph.\"\n",
        "    \"2. Diversity: Include a variety of question types such as factual, analytical, inferential, and evaluative.\"\n",
        "    \"3. Clarity: Make sure each question is clear, concise, and unambiguous.\"\n",
        "    \"4. Complexity: Incorporate questions of varying difficulty levels, from simple recall to complex analysis.\"\n",
        "    \"5. Coverage: Cover the entire content of the paragraph, addressing different sections and key points.\"\n",
        "    \"6. Specificity: Frame questions to be specific and pointed, encouraging precise answers.\"\n",
        "    \"7. Engagement: Create questions that are interesting and engaging, promoting thoughtful responses.\"\n",
        "    \"8. Open-endedness: A portion of the generated questions should encourage creative and thoughtful responses, rather than simple factual recall.\"\n",
        "    \"9. Output: Provide only the five user queries without any introductory or explanatory text.\"\n",
        ")\n",
        "\n",
        "application_description = \"This AI assistant is designed to generate a series of relevant and thought-provoking questions based on the provided context or input. The goal is to generate questions that cover different aspects of the topic without providing answers. The goal is to create an AI that can simulate human-like understanding and reasoning to respond to any query effectively.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_Zd4UPXbMLds"
      },
      "outputs": [],
      "source": [
        "# Defining Instruction Splitter class\n",
        "class InstructionSplitter:\n",
        "  def split_instructions_from_dataset(self, dataset: Dataset):\n",
        "    new_rows = []\n",
        "    for row in dataset:\n",
        "      new_rows.extend(self.split_instructions_from_row(row))\n",
        "    return new_rows\n",
        "\n",
        "  def split_instructions_from_row(self, row):\n",
        "      results = []\n",
        "      for instruction in row['instructions']:\n",
        "          result = row.copy()\n",
        "          result['instruction'] = instruction\n",
        "          del result['instructions']\n",
        "          results.append(result)\n",
        "      return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vkgmlV4NMbTE"
      },
      "outputs": [],
      "source": [
        "class SplitInstructions(Step):\n",
        "    @property\n",
        "    def inputs(self) -> List[str]:\n",
        "        # Specify the input fields expected by this step\n",
        "        return ['instructions']\n",
        "\n",
        "    @property\n",
        "    def outputs(self) -> List[str]:\n",
        "        # Specify the output fields that this step will produce\n",
        "        return ['instruction']\n",
        "\n",
        "    def process(self, inputs: StepInput) -> StepOutput:\n",
        "        inputs = InstructionSplitter().split_instructions_from_dataset(inputs)\n",
        "        yield inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T81KcCVRCWo6"
      },
      "outputs": [],
      "source": [
        "with Pipeline(name=\"Question Generation\") as pipeline:\n",
        "    load_hub_dataset = LoadDataFromHub(\n",
        "        name=\"load_dataset\",\n",
        "        output_mappings={\"prompt\": \"input\"}\n",
        "    )\n",
        "\n",
        "    self_instruct = SelfInstruct(\n",
        "        llm = TransformersLLM(model=\"Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct\", device= \"cuda:0\"),\n",
        "        input_batch_size=1,\n",
        "        add_raw_output=False,\n",
        "        num_instructions=5,\n",
        "        criteria_for_query_generation=criteria_for_query_generation,\n",
        "        application_description=application_description,\n",
        "        output_mappings={\"model_name\": \"instruction_model\"},\n",
        "    )\n",
        "\n",
        "    split_instr = SplitInstructions(\n",
        "        name=\"split_instructions_step\"\n",
        "    )\n",
        "\n",
        "    answer_generation = TextGeneration(\n",
        "        llm = TransformersLLM(model=\"Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct\", device= \"cuda:0\"),\n",
        "        input_batch_size=1,\n",
        "        add_raw_output=False,\n",
        "        output_mappings={\"generation\": \"response\", \"model_name\": \"response_model\"},\n",
        "    )\n",
        "\n",
        "    keep_columns = KeepColumns(\n",
        "        columns = [\"input\", \"instruction\", \"response\", \"instruction_model\", \"response_model\"]\n",
        "    )\n",
        "\n",
        "    load_hub_dataset >> self_instruct >> split_instr >> answer_generation >> keep_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "id": "b2xXOwg-Myps",
        "outputId": "17ac9bc4-bd63-4b21-e394-c5ef3d7b564d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08/25/24 11:34:52] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 💾 Loading `_BatchManager` from cache:             <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#684\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">684</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'/root/.cache/distilabel/pipelines/Question </span>                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Generation/194ec479a666c3548a63e73d0db1d64d6a585d0f/batch_manager.json'</span>    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[08/25/24 11:34:52]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 💾 Loading `_BatchManager` from cache:             \u001b]8;id=386641;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=982038;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#684\u001b\\\u001b[2m684\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32m'/root/.cache/distilabel/pipelines/Question \u001b[0m                               \u001b[2m           \u001b[0m\n",
              "\u001b[2;36m                    \u001b[0m         \u001b[32mGeneration/194ec479a666c3548a63e73d0db1d64d6a585d0f/batch_manager.json'\u001b[0m    \u001b[2m           \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'distilabel.pipeline'</span><span style=\"font-weight: bold\">]</span> 💾 Loaded batch manager from cache doesn't contain <a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#381\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">381</span></a>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         any remaining data. Returning `Distiset` from cache data<span style=\"color: #808000; text-decoration-color: #808000\">...</span>                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[1m[\u001b[0m\u001b[32m'distilabel.pipeline'\u001b[0m\u001b[1m]\u001b[0m 💾 Loaded batch manager from cache doesn't contain \u001b]8;id=420783;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py\u001b\\\u001b[2mbase.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=360848;file:///usr/local/lib/python3.10/dist-packages/distilabel/pipeline/base.py#381\u001b\\\u001b[2m381\u001b[0m\u001b]8;;\u001b\\\n",
              "\u001b[2;36m                    \u001b[0m         any remaining data. Returning `Distiset` from cache data\u001b[33m...\u001b[0m                \u001b[2m           \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "distiset = pipeline.run(\n",
        "    parameters={\n",
        "        load_hub_dataset.name: {\n",
        "            \"repo_id\": \"hassaan-qaisar/initial_prompt\",\n",
        "            \"split\": \"train\",\n",
        "        },\n",
        "        self_instruct.name: {\n",
        "            \"llm\": {\n",
        "                \"generation_kwargs\": {\n",
        "                    \"max_new_tokens\": 256,\n",
        "                    \"temperature\": 0.7,\n",
        "                },\n",
        "            },\n",
        "        },\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ny280qobM4S8",
        "outputId": "544f76b5-d558-4efc-f540-7459d62e21cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Distiset({\n",
            "    default: DatasetDict({\n",
            "        train: Dataset({\n",
            "            features: ['input', 'instruction', 'response', 'instruction_model', 'response_model'],\n",
            "            num_rows: 25\n",
            "        })\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(distiset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5l4E7CnvQa3d",
        "outputId": "0944936e-2229-48f8-ce58-36c78cfca7d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                input  \\\n",
            "0   Renewable energy sources such as solar, wind, ...   \n",
            "1   Gardening is a relaxing and rewarding hobby th...   \n",
            "2   Gardening is a relaxing and rewarding hobby th...   \n",
            "3   Gardening is a relaxing and rewarding hobby th...   \n",
            "4   Gardening is a relaxing and rewarding hobby th...   \n",
            "5   Gardening is a relaxing and rewarding hobby th...   \n",
            "6   Gardening is a relaxing and rewarding hobby th...   \n",
            "7   Gardening is a relaxing and rewarding hobby th...   \n",
            "8   Gardening is a relaxing and rewarding hobby th...   \n",
            "9   Gardening is a relaxing and rewarding hobby th...   \n",
            "10  Gardening is a relaxing and rewarding hobby th...   \n",
            "11  We basically want to make the safe use of an a...   \n",
            "12  We basically want to make the safe use of an a...   \n",
            "13  We basically want to make the safe use of an a...   \n",
            "14  We basically want to make the safe use of an a...   \n",
            "15  We basically want to make the safe use of an a...   \n",
            "16  We basically want to make the safe use of an a...   \n",
            "17  We basically want to make the safe use of an a...   \n",
            "18  We basically want to make the safe use of an a...   \n",
            "19  We basically want to make the safe use of an a...   \n",
            "20  We basically want to make the safe use of an a...   \n",
            "21  We basically want to make the safe use of an a...   \n",
            "22  We basically want to make the safe use of an a...   \n",
            "23  We basically want to make the safe use of an a...   \n",
            "24  We basically want to make the safe use of an a...   \n",
            "\n",
            "                                          instruction  \\\n",
            "0   To help further understand the importance of r...   \n",
            "1   1. What do beginners need to know about garden...   \n",
            "2   2. How to start growing your own plants in a s...   \n",
            "3   3. What important factors should be considered...   \n",
            "4     4. Which plants thrive in low light conditions?   \n",
            "5   5. What is a good combination of plants for a ...   \n",
            "6   6. Do you recommend adding a water feature to ...   \n",
            "7   7. Can I use a floral arrangement in my small ...   \n",
            "8    8. How should I space plants in my small garden?   \n",
            "9   9. Are there any tips for maintaining a small ...   \n",
            "10    10. How do I add more greenery around my house?   \n",
            "11       Here are some basic but relevant AI queries:   \n",
            "12                 ## Question 1 - Analytical Queries   \n",
            "13  * How effective is your AI tool in predicting ...   \n",
            "14  * Is there a specific algorithm or methodology...   \n",
            "15  * Do you have any training data specifically t...   \n",
            "16  * Can you provide an example scenario where yo...   \n",
            "17  * Have you encountered situations where custom...   \n",
            "18                 ## Question 2 - Evaluative Queries   \n",
            "19  * Based on the new regulation in your company,...   \n",
            "20  * How can you ensure that the results of your ...   \n",
            "21  * In case of a significant loss, how will you ...   \n",
            "22                    ## Question 3 - Factual Queries   \n",
            "23  * Which factors influence customer loyalty mos...   \n",
            "24  * How does customer loyalty affect overall mar...   \n",
            "\n",
            "                                             response  \\\n",
            "0   \\nHow to Write a Good Question in English?\\nWr...   \n",
            "1   \\nThis is a sample of a paragraph that has bee...   \n",
            "2   \\nThe following paragraph was originally writt...   \n",
            "3   \\nHow to Write a Good Question in English?\\nWr...   \n",
            "4   \\nHow to Write a Good Question in English\\nWri...   \n",
            "5   \\nThe following paragraph was originally writt...   \n",
            "6   \\nThe following paragraph has been provided as...   \n",
            "7   \\nThe following paragraph was originally writt...   \n",
            "8   \\nHow to Write a Good Question in English?\\nWr...   \n",
            "9   \\nThis is a sample question that can be used t...   \n",
            "10  \\nThe following paragraph was written by an AI...   \n",
            "11  \\nHow to Write a Compelling Open-Ended Questio...   \n",
            "12                                                      \n",
            "13  \\nThe following paragraph was originally writt...   \n",
            "14  \\nHow to Write a Good Question in English?\\nWr...   \n",
            "15  \\nHow to Write a Good Question in English?\\nWr...   \n",
            "16  \\nThis page was last edited on 17 April 2019, ...   \n",
            "17  \\nThe following paragraph was originally writt...   \n",
            "18  \\nThe following paragraph was originally writt...   \n",
            "19  \\nThis is a sample of a question that can be g...   \n",
            "20  \\nHow to Write a Good Open-Ended Question\\nOpe...   \n",
            "21  \\nHow to Write a Good Question in English\\nWri...   \n",
            "22                                                      \n",
            "23  \\nThis is a sample of a paragraph that has bee...   \n",
            "24  \\nThis is a sample of a paragraph that has bee...   \n",
            "\n",
            "                             instruction_model  \\\n",
            "0   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "1   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "2   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "3   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "4   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "5   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "6   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "7   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "8   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "9   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "10  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "11  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "12  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "13  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "14  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "15  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "16  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "17  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "18  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "19  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "20  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "21  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "22  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "23  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "24  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct   \n",
            "\n",
            "                                response_model  \n",
            "0   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "1   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "2   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "3   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "4   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "5   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "6   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "7   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "8   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "9   Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "10  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "11  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "12  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "13  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "14  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "15  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "16  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "17  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "18  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "19  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "20  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "21  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "22  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "23  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n",
            "24  Doctor-Shotgun/TinyLlama-1.1B-32k-Instruct  \n"
          ]
        }
      ],
      "source": [
        "print(distiset['default']['train'].to_pandas())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
